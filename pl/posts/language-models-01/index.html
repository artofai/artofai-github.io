<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <title>Language Models 01 - The Art of Artificial Intelligence</title>
  <meta name="description" content="If there is one single topic I could reccomend to learn for every person learning NLP - that would be language models.
Chances that you will need to implement a LM in your work are very small. But concepts you can learn are priceless. A lot of NLP topics like sequence to sequence models, machine translation, summarization - or even famous transformer models are built on top of lanaguage models. Of course they also have some direct applications."><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "The Art of Artificial Intelligence",
    
    "url": "http:\/\/artofai.github.io\/"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "http:\/\/artofai.github.io\/"
  
  
  
  
}
</script>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "http:\/\/artofai.github.io\/",
          "name": "home"
        }
    },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "http:\/\/artofai.github.io\/pl\/posts\/language-models-01\/",
          "name": "Language models 01"
        }
    }]
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "author": {
    "name" : ""
  },
  "headline": "Language Models 01",
  "description" : "If there is one single topic I could reccomend to learn for every person learning NLP - that would be language models.\nChances that you will need to implement a LM in your work are very small. But concepts you can learn are priceless. A lot of NLP topics like sequence to sequence models, machine translation, summarization - or even famous transformer models are built on top of lanaguage models. Of course they also have some direct applications.",
  "inLanguage" : "en",
  "wordCount":  1130 ,
  "datePublished" : "2021-02-14T12:56:09",
  "dateModified" : "2021-02-14T12:56:09",
  "image" : "http:\/\/artofai.github.io\/",
  "keywords" : [ "nlp, language-models, pytorch" ],
  "mainEntityOfPage" : "http:\/\/artofai.github.io\/pl\/posts\/language-models-01\/",
  "publisher" : {
    "@type": "Organization",
    "name" : "http:\/\/artofai.github.io\/",
    "logo" : {
        "@type" : "ImageObject",
        "url" : "http:\/\/artofai.github.io\/",
        "height" :  60 ,
        "width" :  60
    }
  }
}
</script>

<meta property="og:title" content="Language Models 01" />
<meta property="og:description" content="If there is one single topic I could reccomend to learn for every person learning NLP - that would be language models.
Chances that you will need to implement a LM in your work are very small. But concepts you can learn are priceless. A lot of NLP topics like sequence to sequence models, machine translation, summarization - or even famous transformer models are built on top of lanaguage models. Of course they also have some direct applications.">
<meta property="og:url" content="http://artofai.github.io/pl/posts/language-models-01/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="The Art of Artificial Intelligence" />

  <meta name="twitter:title" content="Language Models 01" />
  <meta name="twitter:description" content="If there is one single topic I could reccomend to learn for every person learning NLP - that would be language models.
Chances that you will need to implement a LM in your work are very small. But â€¦">
  <meta name="twitter:card" content="summary" />
  <meta name="generator" content="Hugo 0.80.0" />
  <link rel="alternate" href="http://artofai.github.io/index.xml" type="application/rss+xml" title="The Art of Artificial Intelligence"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous"><link rel="stylesheet" href="http://artofai.github.io/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" /><link rel="stylesheet" href="http://artofai.github.io/css/syntax.css" /><link rel="stylesheet" href="http://artofai.github.io/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous"><style>

    aside {
        border: rebeccapurple solid;
    }

</style>


  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="http://artofai.github.io/">The Art of Artificial Intelligence</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        

        

        
      </ul>
    </div>

    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  






  

  <header class="header-section ">
    
    <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="pl-heading">
              
                <h1>Language Models 01</h1>
              
              
                <hr class="small">
              
              
              
            </div>
          </div>
        </div>
      </div>
    </div>
  </header>


    
<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        <p>If there is one single topic I could reccomend to learn for every person learning NLP - that would be language models.</p>
<p>Chances that you will need to implement a LM in your work are very small. But concepts you can learn are priceless. A lot of NLP topics like sequence to sequence models, machine translation, summarization - or even famous transformer models are built on top of lanaguage models. Of course they also have some direct applications.</p>
<p>In this series we are going to build a language model using pytorch. We will start with required theory, next we will examine the data, build the model and evaluate it. In the end we will some fun things with trained model!</p>
<h1 id="table-of-contents">Table of contents</h1>
<ol>
<li>Introduction and theory (you are here)</li>
<li>Dataset exploratory analysis</li>
<li>Project structure and tokenizer</li>
<li>Training the model</li>
<li>Evaluation of language model</li>
<li>Experiments with the model</li>
<li>Excerises for you</li>
</ol>
<p>Let&rsquo;s get started!</p>
<h2 id="theory">Theory</h2>
<p>Languge models can be divided into smaller groups. To basic ones are:</p>
<ul>
<li><em>casual language models</em> - also called <em>autoregressive</em></li>
<li><em>masked language models</em> - also called <em>autoencoding</em></li>
</ul>
<p>In this series we will work with the first type - from now by language model I mean a casual one. Masked LM is a seperate topic ;)</p>
<p>We can also divide them by type of tokens:</p>
<ul>
<li>word-level</li>
<li>subword-level</li>
<li>character-level</li>
</ul>
<p>Each of this type makes a different assumption what token is. In this series we will make a character-level one. The main reason is a lot faster training - it will allow you to learn and experiment faster. After completing this tutorial you will be able to create word-level on your own - as mechanics are basically the same.</p>
<h3 id="definition">Definition</h3>
<p>A casual language model is a function that predicts next token in a given sentence. Sound simple, huh? Let&rsquo;s make it a little bit formal.</p>
<p><em>If you are afraid of formal definitions you should not - they really help in advanced scenarions</em></p>
<p>Let&rsquo;s start with <em>sentence</em> - it is a sequence of tokens from vocabulary. First step for language modeling is to define <em>vocabulary</em> \(V\) - simply the list of allowed tokens + some special ones. We already said that tokens can be either words, subwords or characters. We denote sentence as a sequence of tokens:</p>
<p>$$ S = w_0, w_1, w_3, \ldots, w_{n-1}, w_i \isin V $$</p>
<p>The model takes a sequence of tokens and predicts next one. In other words, it predicts a probability for all tokens - just like in regualar classification problem. We denote it as following:</p>
<p>$$ P(w_n | w_0, w_1, w_2, \ldots, w_{n-2}, w_{n-1}) $$</p>
<p>For example, if ask the model &ldquo;What is the next token for sequence: &lsquo;I like &lsquo;&rdquo; we can get:</p>
<ol>
<li>to (0.11)</li>
<li>good (0.05)</li>
<li>it (0.03)</li>
<li>Chopin (0.01)</li>
<li>&hellip;</li>
</ol>
<p>All of them summing up to 1 (to make proper probability distribution).</p>
<p>With the formal notation we can say:</p>
<ol>
<li>\( P(\text{to} | \text{I}, \text{like}) = 0.11 \)</li>
<li>\( P(\text{good} | \text{I}, \text{like}) = 0.05 \)</li>
<li>\( P(\text{it} | \text{I}, \text{like}) = 0.03 \)</li>
<li>\( P(\text{Chopin} | \text{I}, \text{like}) = 0.01 \)</li>
</ol>
<p>We are good to go but there are important corner cases.</p>
<h4 id="special-tokens">Special tokens</h4>
<p>What if we have a sentence that should not have a continuation? For example: &ldquo;The sky is blue.&rdquo; (I even added a full stop to indicate it).</p>
<p>$$ P(w_5 | \text{The}, \text{sky}, \text{is}, \text{blue}, \text{.}) = ? $$</p>
<p>We need a special token to mark end of the sentence. It is usually denoted as <code>&lt;s/&gt;</code>, <code>&lt;eos&gt;</code>, <code>[EOS]</code> or similar.</p>
<ol>
<li>\( P(\text{&lt;s/&gt;} | \text{The}, \text{sky}, \text{is}, \text{blue}, \text{.}) = 0.8 \)</li>
<li>\( P(\text{because} | \text{The}, \text{sky}, \text{is}, \text{blue}, \text{.}) = 0.02 \)</li>
<li>\( P(\text{lyrics} | \text{The}, \text{sky}, \text{is}, \text{blue}, \text{.}) = 0.01 \)</li>
<li><em>all other</em></li>
</ol>
<p>But what for the beginning? What if we ask a model to predict <em>first</em> token in a new sequence? Something like:</p>
<p>$$ P(w_0 | \varnothing) $$</p>
<p>We introduce another special token for it - <code>&lt;s&gt;</code>/<code>&lt;sos&gt;</code>/<code>[BOS]</code> - start/beginning of sequence. So our equation become:</p>
<p>$$ P(w_0 | \text{&lt;s&gt;} ) $$</p>
<p>Introduction of start/end tokens means that for the model we need to add them for all sentences.</p>
<p>$$ S = \text{&lt;s&gt;}, w_1, w_2, w_3, \ldots, w_{n-1}, \text{&lt;/s&gt;} $$</p>
<p>For example</p>
<pre><code>The sky is blue .
</code></pre><p>becomes</p>
<pre><code>&lt;s&gt; The sky is blue . &lt;/s&gt;
</code></pre><p>There can be more special tokens - like <code>&lt;oov&gt;</code>/<code>&lt;unk&gt;</code> for out-of-vocabulary token or <code>&lt;pad&gt;</code> for length padding. We will introduce them later when needed.</p>
<h1 id="sentence-probability">Sentence probability</h1>
<p>It turns out that that we can use same model for computing likelihood of whole sentence. Some examples</p>
<ul>
<li>\(P(\text{Order a taxi please!}) = 0.000000037 \)</li>
<li>\(P(\text{The Earth is flat}) = 0.00000000012 \)</li>
<li>\(P(\text{like pen green bottle}) = 0.0000000000042 \)</li>
<li>\(P(\text{xaaaaaz grrr ee fruu@@}) = 0.00000000000000000000001 \)</li>
</ul>
<p>As you can see, sentences that sound reasonable have bigger scores than trashy ones. Still, absolute numbers are very low. We need to cover <em>all</em> possible sentences. And there are <em>a lot of</em> them.</p>
<p>Advanced</p>
<p>In order to grab a better intuition consider a following language used by Aliens</p>
<ol>
<li>It has 2 000 words (there is about 170 000 words in English)</li>
<li>Sentences has to be maximum 7 words lenght</li>
</ol>
<p>In such scenarion number of possible sentences is:</p>
<p>$$ V = 2000 $$
$$ 1 \le N \le 7 $$
$$ |S| = 2000 + 2000^2 + 2000^3 + \ldots + 2000^7 $$</p>
<p>It gives 128 064 032 016 008 004 002 000 possible sentences!
Assuming that each of them is equaly likely, probability of each of these is 0.0000000000000000000000078 - even for such primitive language.</p>
<p>Fortunaltelly we are more interested in relative likelihoods. For example</p>
<p>$$ P(\text{&ldquo;I have a car&rdquo;}) &gt; P(\text{&ldquo;I has a car&rdquo;}) $$</p>
<p>The model says <em>it is more likely that &ldquo;have&rdquo; is better than &ldquo;has&rdquo; for this sentence</em>.</p>
<p>But how to compute those? Answer is the chain rule. Imagine we have a sentence</p>
<p>$$ S_0 = \text{&lt;s&gt;}, w_1, w_2, w_3, \text{&lt;/s&gt;} $$</p>
<p>In order to compute probability of whole sentence we start with question &ldquo;what is the probability that the sentence starts with with \(w_1\)</p>
<p>$$ P(w_1| \text{&lt;s&gt;})$$</p>
<p>Next, we compute how likely it is to \(\text{&lt;s&gt;}, w_1\) be followed by \(w_2\) and mulpitply it with previous score.</p>
<p>$$ P(w_1| \text{&lt;s&gt;}) \cdot P(w_2 | \text{&lt;s&gt;}, w_1 )$$</p>
<p>Continuing to the next token</p>
<p>$$ P(w_1| \text{&lt;s&gt;}) \cdot P(w_2 | \text{&lt;s&gt;}, w_1 ) \cdot P(w_3 | \text{&lt;s&gt;}, w_1, w_2 ) $$</p>
<p>To the end</p>
<p>$$ P(w_1| \text{&lt;s&gt;}) \cdot P(w_2 | \text{&lt;s&gt;}, w_1 ) \cdot P(w_3 | \text{&lt;s&gt;}, w_1, w_2 ) \cdot P(\text{&lt;/s&gt;} | \text{&lt;s&gt;}, w_1, w_2, w_3 ) $$</p>
<p>In general case it is defined as:</p>
<p>$$ \prod_{i=1}^n P(w_i | w_1, \ldots, w_{i-1}) $$</p>
<aside class="deeper">
    Advanced
    <hr>
    <div>
    

As at each step we multiply numbers in interval \([0, 1]\) at each step the value will be smaller and smaller - approaching zero. Can you see an issue with it?


    </div>

</aside>


        
          <div class="blog-tags">
            
              <a href="http://artofai.github.io//tags/nlp/">nlp</a>&nbsp;
            
              <a href="http://artofai.github.io//tags/language-models/">language-models</a>&nbsp;
            
              <a href="http://artofai.github.io//tags/pytorch/">pytorch</a>&nbsp;
            
          </div>
        

        

        
      </article>

      
        <ul class="pager blog-pager">
          
          
        </ul>
      


      

    </div>
  </div>
</div>

      
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
          
        </ul>
        <p class="credits copyright text-muted">
          

          &nbsp;&bull;&nbsp;&copy;
          
            2021
          

          
            &nbsp;&bull;&nbsp;
            <a href="http://artofai.github.io/">The Art of Artificial Intelligence</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          <a href="https://gohugo.io">Hugo v0.80.0</a> powered &nbsp;&bull;&nbsp; Theme <a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a> adapted from <a href="https://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a>
          
        </p>
      </div>
    </div>
  </div>
</footer><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"></script>
<script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<script src="http://artofai.github.io/js/main.js"></script><script> renderMathInElement(document.body); </script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="http://artofai.github.io/js/load-photoswipe.js"></script>









    
  </body>
</html>

