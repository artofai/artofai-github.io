[{"categories":null,"content":"We start the implementation with dataset exploratory analysis and project template. Project template If you want to code along (I strongly reccomend such aproach) you can use any project template you like. The template I will use is following: Going top-down: char_lm - Directory with model’s package data - All data. It contains multiple subdirectories raw - Data as downloaded/scrapped/extracted. Read only, no manual manipulations allowed! interim - Intermediate formats - after prerocessing etc dataset - Datasets after preparatons, ready to be used by models. models - Storing trained models notebooks - Jupyter notebooks used for exploration/rough experimentation. No other use allowed! scripts - Utility scripts that are not part of model package. Distinction between scripts and char_lm is pretty arbitrary tests - unit tests. Yes we are going to write them. .pre-commit-config.yaml - I use pre-commit to help me keep code quality. Optional poetry.lock/pyproject.toml - I use poetry for installing packages. I encourage you to give a try but it is file to stick with conda/pip You can download complete project from https://github.com/artofai/char-lm Dataset As I stated in previous part we will start with character language model and make a switch to subword later. For now a good dataset is US Census: City and Place Names. Download and extract it to data/raw/us-city-place-names.csv. Spend a few minuts to familiarize yourself with the data and we will go to exploration. Aside Try to perform exploratory analysis on your own and and compare your conclusions with mine! Complete notebook is avaliable on github: https://github.com/artofai/char-lm/blob/master/notebooks/cities-eda.ipynb. ","date":"2021-02-28","objectID":"/posts/language-models-02/:0:0","tags":null,"title":"Language Models 02","uri":"/posts/language-models-02/"},{"categories":null,"content":"EDA First issue is the encoding. If we try to load it with default pd.read_csv settings it will throw a unicode exception. Unfortunatrelly there is no information about encoding on dataset page so I did a small investigation and it looks like ISO-8859-1 (or latin-1 - it’s the same) is correct encoding. If you would like to read more about dealing with encoding issues leave a comment. df = pd.read_csv('../data/raw/us-city-place-names.csv', encoding='ISO-8859-1') df.sample(10) state_id state_name city 40583 45 Texas Leakey 46096 50 West Virginia Pax 36000 39 Ohio Jackson Center village 25542 30 Montana Hot Springs 29099 34 North Carolina Lowland 43780 48 Texas Burton 24541 29 Missouri Ellisville 48214 55 Wisconsin Endeavor village 6706 12 Florida Neptune Beach 25650 31 New Jersey Beverly There are three columns. We are interested only in city. Interestingly, we could also utilize information about state but let’s leave it for another time. There is no nulls (yay) and after deduplicationg city we have about 25000 examples - decent. df.isnull().sum() state_id 0 state_name 0 city 0 dtype: int64 df_deduplicated = df.drop_duplicates('city') len(df_deduplicated) 24583 Analysis of city lengths (in characters) cities = df_deduplicated['city'] lens = [len(s) for s in cities] sns.distplot(lens, rug=True) Distribution looks pretty nice - skewed normal distribution with little tail. Let’s take a look on the longest examples: cities.sort_values(key=lambda s: s.str.len())[-20:] 12555 South Chicago Heights village 31898 Village of the Branch village 29685 Peapack and Gladstone borough 48228 Fontana-on-Geneva Lake village 17285 Lexington-Fayette urban county 25161 Village of Four Seasons village 4160 El Paso de Robles (Paso Robles) 22113 Village of Grosse Pointe Shores 20047 Chevy Chase Section Five village 20048 Chevy Chase Section Three village 7642 Webster County unified government 30225 Los Ranchos de Albuquerque village 7293 Echols County consolidated government 16237 Greeley County unified government (balance) 7327 Georgetown-Quitman County unified government 7254 Cusseta-Chattahoochee County unified government 7151 Athens-Clarke County unified government (balance) 42659 Nashville-Davidson metropolitan government (balance) 17293 Louisville/Jefferson County metro government (balance) 7155 Augusta-Richmond County consolidated government (balance) Name: city, dtype: object I don’t live in the US so names with (balance) are a mystery. I will remove them later. Next, let’s take a look on character distributions: char_freq = Counter(chain.from_iterable(cities)) sorted(char_freq.items(), key=lambda x: x[1]) [('ñ', 3), ('/', 3), ('X', 4), ('(', 15), (')', 15), (\"'\", 27), ('j', 52), ('Z', 61), ('Q', 68), ('-', 84), ('.', 108), ('Y', 134), ('q', 153), ('U', 168), ('z', 295), ('x', 364), ('J', 406), ('I', 424), ('K', 653), ('V', 741), ('O', 762), ('T', 1042), ('D', 1069), ('E', 1110), ('N', 1134), ('A', 1233), ('F', 1300), ('f', 1362), ('G', 1444), ('R', 1551), ('W', 1714), ('L', 2002), ('H', 2075), ('P', 2166), ('p', 2257), ('M', 2564), ('B', 2647), ('w', 2708), ('S', 3132), ('m', 3292), ('k', 3484), ('C', 3518), ('b', 3550), ('y', 3772), ('c', 3874), ('d', 5645), ('h', 5900), ('u', 6361), ('v', 6445), ('g', 8222), ('s', 9229), ('t', 11389), (' ', 12847), ('n', 15172), ('r', 15726), ('i', 16546), ('o', 17874), ('a', 21525), ('l', 22549), ('e', 25270)] Looks good, except character ñ and /. ","date":"2021-02-28","objectID":"/posts/language-models-02/:1:0","tags":null,"title":"Language Models 02","uri":"/posts/language-models-02/"},{"categories":null,"content":"Removing outliers According to our analysis, we are going to remove entries containing any of ()ñ/. idx_to_drop = cities.str.contains(r'ñ|\\/|\\(|\\)') print(idx_to_drop.sum()) print(cities[idx_to_drop]) cleaned_cities = cities[~idx_to_drop] cleaned_cities 3299 Fredonia (Biscoe) 4160 El Paso de Robles (Paso Robles) 4235 La Cañada Flintridge 4398 San Buenaventura (Ventura) 4874 Cañon City 5051 Raymer (New Raymer) 5130 Milford (balance) 7151 Athens-Clarke County unified government (balance) 7155 Augusta-Richmond County consolidated government (balance) 13788 Indianapolis (balance) 16237 Greeley County unified government (balance) 17293 Louisville/Jefferson County metro government (balance) 25497 Butte-Silver Bow (balance) 30199 Española 40771 Naval Air Station/ Jrb 42575 Hartsville/Trousdale County 42659 Nashville-Davidson metropolitan government (balance) 47809 Addison (Webster Springs) 47820 Bath (Berkeley Springs) 48039 Womelsdorf (Coalton) Name: city, dtype: object It is always good to make a sanity check - how much data will be removed and what. THis operation will remove 20 examples - for me removal list looks great. At this point I stop the EDA. In cleaned_cities there is a list of cities used for modeling. ","date":"2021-02-28","objectID":"/posts/language-models-02/:1:1","tags":null,"title":"Language Models 02","uri":"/posts/language-models-02/"},{"categories":null,"content":"Build dataset script We could in theory save dataset from exploratory notebook. But it would be a bad practice - data enbginerring and exploration shouln’t be mixed. Because of this let’s create a data preprocessing script in scrips/build_dataset.py Our knowledge from EDA can be trasnferred to a function. We already know what kind of preprocessing want to apply so implementation is straightforward: def preprocess_dataset(df: pd.DataFrame) -\u003e List[str]: \"\"\"Preprocesses the cities dataset by removing entries with invalid characters Args: df (pd.DataFrame): Source dataframe, requires column \"city\" Returns: List[str]: A list of cleaned cities \"\"\" df = df.drop_duplicates(\"city\") logger.info(f\"Rows after deduplication: {len(df)}\") cities = df[\"city\"] idx_to_drop = cities.str.contains(r\"ñ|\\/|\\(|\\)\") cities = cities[~idx_to_drop] logger.info(f\"Dropped {idx_to_drop.sum()} outliers\") return list(cities) Second piece, is to take cleaned cities, split them into train/val/test and save into separate files. I also save the full dataset at this step - maybe it will be useful later. To make the process deterministic a random state can be provided. Alert Ability to reproduce your results is important, consider every place where using random generatores saving seed value def split_and_save(cities: List[str], out_dir: Path, random_state: int) -\u003e None: \"\"\"Splits dataset into train/val/test and saves to text files Args: cities (List[str]): List of cleaned cities out_dir (Path): Output directory. Will be created if not exists random_state (int): Random state for splitting \"\"\" out_dir.mkdir(parents=True, exist_ok=True) logger.info(f\"Saving results to {out_dir}\") with (out_dir / \"full.txt\").open(\"wt\", encoding=\"utf-8\") as f: f.write(\"\\n\".join(cities)) keeep_set, test_set = train_test_split( cities, test_size=0.2, random_state=random_state ) train_set, val_set = train_test_split( keeep_set, test_size=0.2, random_state=random_state + 1 ) with (out_dir / \"train.txt\").open(\"wt\", encoding=\"utf-8\") as f: f.write(\"\\n\".join(train_set)) with (out_dir / \"val.txt\").open(\"wt\", encoding=\"utf-8\") as f: f.write(\"\\n\".join(val_set)) with (out_dir / \"test.txt\").open(\"wt\", encoding=\"utf-8\") as f: f.write(\"\\n\".join(test_set)) Building blocks are ready now we can create a simple command line interface using click. We would like to pass the raw, input file, the output directory, specify encoding of the input file and a random state. @click.command() @click.option(\"--input-path\", \"-i\", required=True) @click.option(\"--output-dir\", \"-o\", required=True) @click.option(\"--encoding\", default=\"ISO-8859-1\") @click.option(\"--random-state\", type=int, default=42) def main(input_path: str, output_dir: str, encoding: str, random_state=42): logging.basicConfig(level=logging.INFO) input_path = Path(input_path) output_dir = Path(output_dir) logger.info(f\"Reading file {input_path} with encoding {encoding}\") df = pd.read_csv(input_path, encoding=encoding) logger.info(f\"Read {len(df)} rows\") cities = preprocess_dataset(df) split_and_save(cities, output_dir, random_state) logger.info(\"Done.\") if __name__ == \"__main__\": main() ","date":"2021-02-28","objectID":"/posts/language-models-02/:1:2","tags":null,"title":"Language Models 02","uri":"/posts/language-models-02/"},{"categories":null,"content":"If there is one single topic I could reccomend to learn for every person learning NLP - that would be language models. Chances that you will need to implement a LM in your work are very small. But concepts you can learn are priceless. A lot of NLP topics like sequence to sequence models, machine translation, summarization - or even famous transformer models are built on top of lanaguage models. Of course they also have some direct applications. In this series we are going to build a language model using pytorch. We will start with required theory, next we will examine the data, build the model and evaluate it. In the end we will some fun things with trained model! Series This post is a past of a longer series about casual lanugage modelling. The aim is to provide you all required theory and help you to understand the topic by acutally implementing a language model. We will start with the simples possible and gradually expand it. Table of contents Introduction and theory (you are here) Dataset exploratory analysis Project structure and tokenizer Training the model Evaluation of language model Experiments with the model Excerises for you Let’s get started! ","date":"2021-02-14","objectID":"/posts/language-models-01/:0:0","tags":["nlp","language-models","pytorch"],"title":"Language Models 01","uri":"/posts/language-models-01/"},{"categories":null,"content":"Theory Languge models can be divided into smaller groups. To basic ones are: casual language models - also called autoregressive masked language models - also called autoencoding In this series we will work with the first type - from now by language model I mean a casual one. Masked LM is a seperate topic ;) We can also divide them by type of tokens: word-level subword-level character-level Each of this type makes a different assumption what token is. In this series we will make a character-level one. The main reason is a lot faster training - it will allow you to learn and experiment faster. After completing this tutorial you will be able to create word-level on your own - as mechanics are basically the same. ","date":"2021-02-14","objectID":"/posts/language-models-01/:1:0","tags":["nlp","language-models","pytorch"],"title":"Language Models 01","uri":"/posts/language-models-01/"},{"categories":null,"content":"Definition A casual language model is a function that predicts next token in a given sentence. Sound simple, huh? Let’s make it a little bit formal. If you are afraid of formal definitions you should not - they really help in advanced scenarions Let’s start with sentence - it is a sequence of tokens from vocabulary. First step for language modeling is to define vocabulary \\(V\\) - simply the list of allowed tokens + some special ones. We already said that tokens can be either words, subwords or characters. We denote sentence as a sequence of tokens: $$ S = w_0, w_1, w_3, \\ldots, w_{n-1}, w_i \\isin V $$ The model takes a sequence of tokens and predicts next one. In other words, it predicts a probability for all tokens - just like in regualar classification problem. We denote it as following: $$ P(w_n | w_0, w_1, w_2, \\ldots, w_{n-2}, w_{n-1}) $$ For example, if ask the model “What is the next token for sequence: ‘I like ‘” we can get: to (0.11) good (0.05) it (0.03) Chopin (0.01) … All of them summing up to 1 (to make proper probability distribution). With the formal notation we can say: \\( P(\\text{to} | \\text{I}, \\text{like}) = 0.11 \\) \\( P(\\text{good} | \\text{I}, \\text{like}) = 0.05 \\) \\( P(\\text{it} | \\text{I}, \\text{like}) = 0.03 \\) \\( P(\\text{Chopin} | \\text{I}, \\text{like}) = 0.01 \\) We are good to go but there are important corner cases. Special tokens What if we have a sentence that should not have a continuation? For example: “The sky is blue.” (I even added a full stop to indicate it). $$ P(w_5 | \\text{The}, \\text{sky}, \\text{is}, \\text{blue}, \\text{.}) = ? $$ We need a special token to mark end of the sentence. It is usually denoted as \u003cs/\u003e, \u003ceos\u003e, [EOS] or similar. \\( P(\\text{\u003cs/\u003e} | \\text{The}, \\text{sky}, \\text{is}, \\text{blue}, \\text{.}) = 0.8 \\) \\( P(\\text{because} | \\text{The}, \\text{sky}, \\text{is}, \\text{blue}, \\text{.}) = 0.02 \\) \\( P(\\text{lyrics} | \\text{The}, \\text{sky}, \\text{is}, \\text{blue}, \\text{.}) = 0.01 \\) all other But what for the beginning? What if we ask a model to predict first token in a new sequence? Something like: $$ P(w_0 | \\varnothing) $$ We introduce another special token for it - \u003cs\u003e/\u003csos\u003e/[BOS] - start/beginning of sequence. So our equation become: $$ P(w_0 | \\text{\u003cs\u003e} ) $$ Introduction of start/end tokens means that for the model we need to add them for all sentences. $$ S = \\text{\u003cs\u003e}, w_1, w_2, w_3, \\ldots, w_{n-1}, \\text{\u003c/s\u003e} $$ For example The sky is blue . becomes \u003cs\u003e The sky is blue . \u003c/s\u003e There can be more special tokens - like \u003coov\u003e/\u003cunk\u003e for out-of-vocabulary token or \u003cpad\u003e for length padding. We will introduce them later when needed. Sentence probability It turns out that that we can use same model for computing likelihood of whole sentence. Some examples \\(P(\\text{Order a taxi please!}) = 0.000000037 \\) \\(P(\\text{The Earth is flat}) = 0.00000000012 \\) \\(P(\\text{like pen green bottle}) = 0.0000000000042 \\) \\(P(\\text{xaaaaaz grrr ee fruu@@}) = 0.00000000000000000000001 \\) As you can see, sentences that sound reasonable have bigger scores than trashy ones. Still, absolute numbers are very low. We need to cover all possible sentences. And there are a lot of them. Advanced In order to grab a better intuition consider a following language used by Aliens It has 2 000 words (there is about 170 000 words in English) Sentences has to be maximum 7 words lenght In such scenarion number of possible sentences is: $$ V = 2000 $$ $$ 1 \\le N \\le 7 $$ $$ |S| = 2000 + 2000^2 + 2000^3 + \\ldots + 2000^7 $$ It gives 128 064 032 016 008 004 002 000 possible sentences! Assuming that each of them is equaly likely, probability of each of these is 0.0000000000000000000000078 - even for such primitive language. Fortunaltelly we are more interested in relative likelihoods. For example $$ P(\\text{“I have a car”}) \u003e P(\\text{“I has a car”}) $$ The model says it is more likely that “have” is better than “has” for this sentence. But how to compute those? Answer is the chain rule. Imagine we have a senten","date":"2021-02-14","objectID":"/posts/language-models-01/:1:1","tags":["nlp","language-models","pytorch"],"title":"Language Models 01","uri":"/posts/language-models-01/"}]